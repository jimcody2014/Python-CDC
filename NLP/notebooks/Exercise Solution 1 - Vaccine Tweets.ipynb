{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-0\">Instructions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-data-from-kaggle.com\" data-toc-modified-id=\"Get-data-from-kaggle.com-0.1\">Get data from kaggle.com</a></span></li><li><span><a href=\"#Load-a-dataframe\" data-toc-modified-id=\"Load-a-dataframe-0.2\">Load a dataframe</a></span></li><li><span><a href=\"#Basic-pre-processing\" data-toc-modified-id=\"Basic-pre-processing-0.3\">Basic pre-processing</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. Load this data set from kaggle - kaggle datasets download -d gpreda/pfizer-vaccine-tweets\n",
    "2. Determine the shape of the dataframe\n",
    "3. Review the data types\n",
    "4. Drop the id column\n",
    "5. Check for null values\n",
    "6. Perform the following pre-processing on the 'text' column. \n",
    "    - (new column1) change all text to lowercase\n",
    "    - (new column2) use new column1 and remove contractions.  \n",
    "    - (new column3) use new column2 and string the data back together\n",
    "    - (new column4) use new column3 and tokenize into sentences\n",
    "    - (new column5) use new column3, again, and tokenize into words   \n",
    "    - (new column6) use new column5 and special characters\n",
    "    - (new column7) use new column6 and remove stop words\n",
    "    - (new column8) use new column7 and perform stemming\n",
    "    - (new column9) use new column8 and perform lemmanization\n",
    "    - add columns tweet length and tweet word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from kaggle.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "\n",
    "## Upload your kaggle json file (API Token)\n",
    "#files.upload()\n",
    "\n",
    "#!mkdir ~/.kaggle\n",
    "\n",
    "#!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "#!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d gpreda/pfizer-vaccine-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir data\n",
    "\n",
    "#!unzip zip file name -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -l data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "# What other imports are required?\n",
    "#!pip install contractions\n",
    "#!pip install pyspellchecker\n",
    "\n",
    "import contractions\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfz = pd.read_csv('/Users/jimcody/Documents/2021Python/nlp/data/vaccination_tweets.csv')\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pfz.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "drop_columns = {'id'}\n",
    "pfz = pfz.drop(columns = drop_columns)\n",
    "#pfz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change text to lowercase\n",
    "pfz['lower'] = pfz['text'].str.lower()\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove contractions\n",
    "pfz['remove_ctr'] = pfz['lower'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change no_contract back to a string\n",
    "pfz[\"review_new\"] = [' '.join(map(str, l)) for l in pfz['remove_ctr']]\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenized sentences\n",
    "pfz['tokenized_sent'] = pfz['review_new'].apply(sent_tokenize)\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenized words\n",
    "pfz['tokenized_word'] = pfz['review_new'].apply(word_tokenize)\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "punc = string.punctuation\n",
    "pfz['no_punc'] = pfz['tokenized_word'].apply(lambda x: [word for word in x if word not in punc])\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfz['no_stopwords'] = pfz['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfz['pos_tags'] = pfz['no_stopwords'].apply(nltk.tag.pos_tag)\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfz['wordnet_pos'] = pfz['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "pfz['lemmatized'] = pfz['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "#pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfz['review_len'] = pfz['text'].astype(str).apply(len)\n",
    "pfz['word_count'] = pfz['text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>...</th>\n",
       "      <th>review_new</th>\n",
       "      <th>tokenized_sent</th>\n",
       "      <th>tokenized_word</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>review_len</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rachel Roh</td>\n",
       "      <td>La Crescenta-Montrose, CA</td>\n",
       "      <td>Aggregator of Asian American news; scanning di...</td>\n",
       "      <td>2009-04-08 17:52:46</td>\n",
       "      <td>405</td>\n",
       "      <td>1692</td>\n",
       "      <td>3247</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>Same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>...</td>\n",
       "      <td>same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>[same folks said daikon paste could treat a cy...</td>\n",
       "      <td>[same, folks, said, daikon, paste, could, trea...</td>\n",
       "      <td>[same, folks, said, daikon, paste, could, trea...</td>\n",
       "      <td>[folks, said, daikon, paste, could, treat, cyt...</td>\n",
       "      <td>[(folks, NNS), (said, VBD), (daikon, JJ), (pas...</td>\n",
       "      <td>[(folks, n), (said, v), (daikon, a), (paste, n...</td>\n",
       "      <td>[folk, say, daikon, paste, could, treat, cytok...</td>\n",
       "      <td>97</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albert Fong</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Marketing dude, tech geek, heavy metal &amp; '80s ...</td>\n",
       "      <td>2009-09-21 15:27:30</td>\n",
       "      <td>834</td>\n",
       "      <td>666</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>While the world has been on the wrong side of ...</td>\n",
       "      <td>...</td>\n",
       "      <td>while the world has been on the wrong side of ...</td>\n",
       "      <td>[while the world has been on the wrong side of...</td>\n",
       "      <td>[while, the, world, has, been, on, the, wrong,...</td>\n",
       "      <td>[while, the, world, has, been, on, the, wrong,...</td>\n",
       "      <td>[world, wrong, side, history, year, hopefully,...</td>\n",
       "      <td>[(world, NN), (wrong, JJ), (side, NN), (histor...</td>\n",
       "      <td>[(world, n), (wrong, a), (side, n), (history, ...</td>\n",
       "      <td>[world, wrong, side, history, year, hopefully,...</td>\n",
       "      <td>140</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eli🇱🇹🇪🇺👌</td>\n",
       "      <td>Your Bed</td>\n",
       "      <td>heil, hydra 🖐☺</td>\n",
       "      <td>2020-06-25 23:30:28</td>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "      <td>155</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:33:45</td>\n",
       "      <td>#coronavirus #SputnikV #AstraZeneca #PfizerBio...</td>\n",
       "      <td>...</td>\n",
       "      <td>#coronavirus #sputnikv #astrazeneca #pfizerbio...</td>\n",
       "      <td>[#coronavirus #sputnikv #astrazeneca #pfizerbi...</td>\n",
       "      <td>[#, coronavirus, #, sputnikv, #, astrazeneca, ...</td>\n",
       "      <td>[coronavirus, sputnikv, astrazeneca, pfizerbio...</td>\n",
       "      <td>[coronavirus, sputnikv, astrazeneca, pfizerbio...</td>\n",
       "      <td>[(coronavirus, NN), (sputnikv, NN), (astrazene...</td>\n",
       "      <td>[(coronavirus, n), (sputnikv, n), (astrazeneca...</td>\n",
       "      <td>[coronavirus, sputnikv, astrazeneca, pfizerbio...</td>\n",
       "      <td>140</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charles Adler</td>\n",
       "      <td>Vancouver, BC - Canada</td>\n",
       "      <td>Hosting \"CharlesAdlerTonight\" Global News Radi...</td>\n",
       "      <td>2008-09-10 11:28:53</td>\n",
       "      <td>49165</td>\n",
       "      <td>3933</td>\n",
       "      <td>21853</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:23:59</td>\n",
       "      <td>Facts are immutable, Senator, even when you're...</td>\n",
       "      <td>...</td>\n",
       "      <td>facts are immutable, senator, even when you ar...</td>\n",
       "      <td>[facts are immutable, senator, even when you a...</td>\n",
       "      <td>[facts, are, immutable, ,, senator, ,, even, w...</td>\n",
       "      <td>[facts, are, immutable, senator, even, when, y...</td>\n",
       "      <td>[facts, immutable, senator, even, ethically, s...</td>\n",
       "      <td>[(facts, NNS), (immutable, JJ), (senator, NN),...</td>\n",
       "      <td>[(facts, n), (immutable, a), (senator, n), (ev...</td>\n",
       "      <td>[fact, immutable, senator, even, ethically, st...</td>\n",
       "      <td>140</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Citizen News Channel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Citizen News Channel bringing you an alternati...</td>\n",
       "      <td>2020-04-23 17:58:42</td>\n",
       "      <td>152</td>\n",
       "      <td>580</td>\n",
       "      <td>1473</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:17:19</td>\n",
       "      <td>Explain to me again why we need a vaccine @Bor...</td>\n",
       "      <td>...</td>\n",
       "      <td>explain to me again why we need a vaccine @bor...</td>\n",
       "      <td>[explain to me again why we need a vaccine @bo...</td>\n",
       "      <td>[explain, to, me, again, why, we, need, a, vac...</td>\n",
       "      <td>[explain, to, me, again, why, we, need, a, vac...</td>\n",
       "      <td>[explain, need, vaccine, borisjohnson, matthan...</td>\n",
       "      <td>[(explain, RB), (need, JJ), (vaccine, NN), (bo...</td>\n",
       "      <td>[(explain, r), (need, a), (vaccine, n), (boris...</td>\n",
       "      <td>[explain, need, vaccine, borisjohnson, matthan...</td>\n",
       "      <td>135</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_name              user_location  \\\n",
       "0            Rachel Roh  La Crescenta-Montrose, CA   \n",
       "1           Albert Fong          San Francisco, CA   \n",
       "2              eli🇱🇹🇪🇺👌                   Your Bed   \n",
       "3         Charles Adler     Vancouver, BC - Canada   \n",
       "4  Citizen News Channel                        NaN   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Aggregator of Asian American news; scanning di...  2009-04-08 17:52:46   \n",
       "1  Marketing dude, tech geek, heavy metal & '80s ...  2009-09-21 15:27:30   \n",
       "2                                     heil, hydra 🖐☺  2020-06-25 23:30:28   \n",
       "3  Hosting \"CharlesAdlerTonight\" Global News Radi...  2008-09-10 11:28:53   \n",
       "4  Citizen News Channel bringing you an alternati...  2020-04-23 17:58:42   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0             405          1692             3247          False   \n",
       "1             834           666              178          False   \n",
       "2              10            88              155          False   \n",
       "3           49165          3933            21853           True   \n",
       "4             152           580             1473          False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2020-12-20 06:06:44  Same folks said daikon paste could treat a cyt...   \n",
       "1  2020-12-13 16:27:13  While the world has been on the wrong side of ...   \n",
       "2  2020-12-12 20:33:45  #coronavirus #SputnikV #AstraZeneca #PfizerBio...   \n",
       "3  2020-12-12 20:23:59  Facts are immutable, Senator, even when you're...   \n",
       "4  2020-12-12 20:17:19  Explain to me again why we need a vaccine @Bor...   \n",
       "\n",
       "   ...                                         review_new  \\\n",
       "0  ...  same folks said daikon paste could treat a cyt...   \n",
       "1  ...  while the world has been on the wrong side of ...   \n",
       "2  ...  #coronavirus #sputnikv #astrazeneca #pfizerbio...   \n",
       "3  ...  facts are immutable, senator, even when you ar...   \n",
       "4  ...  explain to me again why we need a vaccine @bor...   \n",
       "\n",
       "                                      tokenized_sent  \\\n",
       "0  [same folks said daikon paste could treat a cy...   \n",
       "1  [while the world has been on the wrong side of...   \n",
       "2  [#coronavirus #sputnikv #astrazeneca #pfizerbi...   \n",
       "3  [facts are immutable, senator, even when you a...   \n",
       "4  [explain to me again why we need a vaccine @bo...   \n",
       "\n",
       "                                      tokenized_word  \\\n",
       "0  [same, folks, said, daikon, paste, could, trea...   \n",
       "1  [while, the, world, has, been, on, the, wrong,...   \n",
       "2  [#, coronavirus, #, sputnikv, #, astrazeneca, ...   \n",
       "3  [facts, are, immutable, ,, senator, ,, even, w...   \n",
       "4  [explain, to, me, again, why, we, need, a, vac...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [same, folks, said, daikon, paste, could, trea...   \n",
       "1  [while, the, world, has, been, on, the, wrong,...   \n",
       "2  [coronavirus, sputnikv, astrazeneca, pfizerbio...   \n",
       "3  [facts, are, immutable, senator, even, when, y...   \n",
       "4  [explain, to, me, again, why, we, need, a, vac...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  [folks, said, daikon, paste, could, treat, cyt...   \n",
       "1  [world, wrong, side, history, year, hopefully,...   \n",
       "2  [coronavirus, sputnikv, astrazeneca, pfizerbio...   \n",
       "3  [facts, immutable, senator, even, ethically, s...   \n",
       "4  [explain, need, vaccine, borisjohnson, matthan...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(folks, NNS), (said, VBD), (daikon, JJ), (pas...   \n",
       "1  [(world, NN), (wrong, JJ), (side, NN), (histor...   \n",
       "2  [(coronavirus, NN), (sputnikv, NN), (astrazene...   \n",
       "3  [(facts, NNS), (immutable, JJ), (senator, NN),...   \n",
       "4  [(explain, RB), (need, JJ), (vaccine, NN), (bo...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [(folks, n), (said, v), (daikon, a), (paste, n...   \n",
       "1  [(world, n), (wrong, a), (side, n), (history, ...   \n",
       "2  [(coronavirus, n), (sputnikv, n), (astrazeneca...   \n",
       "3  [(facts, n), (immutable, a), (senator, n), (ev...   \n",
       "4  [(explain, r), (need, a), (vaccine, n), (boris...   \n",
       "\n",
       "                                          lemmatized review_len word_count  \n",
       "0  [folk, say, daikon, paste, could, treat, cytok...         97         12  \n",
       "1  [world, wrong, side, history, year, hopefully,...        140         21  \n",
       "2  [coronavirus, sputnikv, astrazeneca, pfizerbio...        140         15  \n",
       "3  [fact, immutable, senator, even, ethically, st...        140         20  \n",
       "4  [explain, need, vaccine, borisjohnson, matthan...        135         14  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11003 entries, 0 to 11002\n",
      "Data columns (total 27 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   user_name         11003 non-null  object\n",
      " 1   user_location     8734 non-null   object\n",
      " 2   user_description  10323 non-null  object\n",
      " 3   user_created      11003 non-null  object\n",
      " 4   user_followers    11003 non-null  int64 \n",
      " 5   user_friends      11003 non-null  int64 \n",
      " 6   user_favourites   11003 non-null  int64 \n",
      " 7   user_verified     11003 non-null  bool  \n",
      " 8   date              11003 non-null  object\n",
      " 9   text              11003 non-null  object\n",
      " 10  hashtags          8426 non-null   object\n",
      " 11  source            11002 non-null  object\n",
      " 12  retweets          11003 non-null  int64 \n",
      " 13  favorites         11003 non-null  int64 \n",
      " 14  is_retweet        11003 non-null  bool  \n",
      " 15  lower             11003 non-null  object\n",
      " 16  remove_ctr        11003 non-null  object\n",
      " 17  review_new        11003 non-null  object\n",
      " 18  tokenized_sent    11003 non-null  object\n",
      " 19  tokenized_word    11003 non-null  object\n",
      " 20  no_punc           11003 non-null  object\n",
      " 21  no_stopwords      11003 non-null  object\n",
      " 22  pos_tags          11003 non-null  object\n",
      " 23  wordnet_pos       11003 non-null  object\n",
      " 24  lemmatized        11003 non-null  object\n",
      " 25  review_len        11003 non-null  int64 \n",
      " 26  word_count        11003 non-null  int64 \n",
      "dtypes: bool(2), int64(7), object(18)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "pfz.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
