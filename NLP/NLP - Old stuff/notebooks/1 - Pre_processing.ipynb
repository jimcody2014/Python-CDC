{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Process-Overview\" data-toc-modified-id=\"Process-Overview-1\">Process Overview</a></span></li><li><span><a href=\"#Terminology\" data-toc-modified-id=\"Terminology-2\">Terminology</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-quick-overview-of-common-pre-processing-tasks\" data-toc-modified-id=\"A-quick-overview-of-common-pre-processing-tasks-2.1\">A quick overview of common pre-processing tasks</a></span></li><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-2.2\">Objective</a></span></li><li><span><a href=\"#Pre-requisite-work\" data-toc-modified-id=\"Pre-requisite-work-2.3\">Pre-requisite work</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-data-into-a-dataframe\" data-toc-modified-id=\"Load-the-data-into-a-dataframe-2.3.1\">Load the data into a dataframe</a></span></li></ul></li><li><span><a href=\"#Pre-processing\" data-toc-modified-id=\"Pre-processing-2.4\">Pre-processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Contractions\" data-toc-modified-id=\"Contractions-2.4.1\">Contractions</a></span><ul class=\"toc-item\"><li><span><a href=\"#An-alternative---Regular-Expressions\" data-toc-modified-id=\"An-alternative---Regular-Expressions-2.4.1.1\">An alternative - Regular Expressions</a></span></li></ul></li><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-2.4.2\">Tokenization</a></span></li><li><span><a href=\"#Noise-cleaning-(spacing,-lowercase,-special-characters)\" data-toc-modified-id=\"Noise-cleaning-(spacing,-lowercase,-special-characters)-2.4.3\">Noise cleaning (spacing, lowercase, special characters)</a></span></li><li><span><a href=\"#Use-the-string-package\" data-toc-modified-id=\"Use-the-string-package-2.4.4\">Use the string package</a></span></li><li><span><a href=\"#Stop-words\" data-toc-modified-id=\"Stop-words-2.4.5\">Stop words</a></span></li><li><span><a href=\"#Stemming/-Lemmanization\" data-toc-modified-id=\"Stemming/-Lemmanization-2.4.6\">Stemming/ Lemmanization</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology\n",
    "\n",
    "- Tokenization:\n",
    "    - A sentence contain be broken down in elements or units of semantics.  These are referred to as tokens.  Tokens can be words, numbers, symbols, punctuation marks, etc. The process of generating tokens is called tokenization.\n",
    "- Stop words:\n",
    "    - Stop words are the most common words in a natural language.  For the purposes of analyzing text data, stop words do not, generally, add value and are removed from the corpus.\n",
    "- Stemming:\n",
    "    - Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "- Lemmatization:\n",
    "    - Lemmatization is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**punkt**   Punkt Sentence Tokenizer\n",
    "\n",
    "- This tokenizer divides a text into a list of sentences\n",
    "by using an unsupervised algorithm to build a model for abbreviation\n",
    "words, collocations, and words that start sentences.  It must be\n",
    "trained on a large collection of plaintext in the target language\n",
    "before it can be used.\n",
    "\n",
    "- The NLTK data package includes a pre-trained Punkt tokenizer for\n",
    "English.\n",
    "\n",
    "**stopwords**\n",
    "\n",
    "- The stopwords are a list of words that are very very common but don’t provide useful information for most text analysis procedures.\n",
    "- Foir example, “the”, “a”, “an”, “in” \n",
    "\n",
    "**wordnet**\n",
    "\n",
    "- WordNet is a database of words in the English language.\n",
    "- It is organized by concept and meaning\n",
    "- WordNet is a network of words linked by lexical and semantic relations. \n",
    "    - Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms, called synsets, each expressing a distinct concept. \n",
    "    - Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the WordNet browser.\n",
    "- See this site for a great explanation: https://devopedia.org/wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob('Do not taste or eat raw dough or batter that is meant to be baked or cooked. This includes dough or batter for cookies, cakes, pies, biscuits, pancakes, tortillas, pizza, or crafts. Do not let children taste raw dough or batter or play with dough at home or in restaurants.')\n",
    "print(blob.words)\n",
    "print(' ')\n",
    "print(blob.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(\"Spellling whil typing is hardd for me\")\n",
    "blob_corrected  = blob.correct()\n",
    "print(blob_corrected.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_word = Word(\"disease\")\n",
    "print(nlp_word.definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick overview of common pre-processing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    # This is the python built-in version\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re    # This is the python built-in version of regex\n",
    "#nltk.data.path.append(os.path.join(os.getcwd(), \"nltk_data\"))\n",
    "\n",
    "# Sample text - from CDC website\n",
    "text = 'Minnesota officials found E. coli O157:H7 in a package of leftover Josie’s Organics baby spinach collected from a sick person’s home. Five people in this outbreak reported eating spinach in the week before they got sick and 1 reported Josie’s Organics brand.'\n",
    "print('Original Text -------------------',text)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Length of Text ------------------',len(text))\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tokenization in NLP is the process by which a large quantity of text is divided \n",
    "# into smaller parts called tokens.\n",
    "\n",
    "# Split text into words using NLTK\n",
    "words = word_tokenize(text)\n",
    "print('Tokenized words -------------------',words)\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print('Tokenized sentences -------------------',sentences)\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "# List stop words - Words that do not contribute to the meaning of a phrase\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('stop words -------------------',stop_words)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('no stopwords -------------------',stopwords.words(\"english\"))\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "\n",
    "# Reset text\n",
    "text = \"What is public health? It's about more than just responding to disease outbreaks. The standard definition of public health used for over 100 year was developed by C.-E.A. Winslow. Is this a good definition to keep using?\"\n",
    "\n",
    "\n",
    "# Normalize it\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "# Tokenize it\n",
    "words = text.split()\n",
    "print('lowercase & tokenized -------------------',words)\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "\n",
    "print('no stop words -------------------',words)\n",
    "print(' ')\n",
    "print(' ')\n",
    "    \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# different forms of the same \"word\"\n",
    "input1 = 'List listed lists listing listings'\n",
    "words1 = input1.lower().split(' ')\n",
    "words1\n",
    "porter = nltk.PorterStemmer()\n",
    "print([porter.stem(t) for t in words1])\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print('stemmed -------------------',stemmed)\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print('lemmed -------------------',lemmed)\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "# Lemmatize verbs by specifying pos\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "print('lemmed verbs -------------------',lemmed)\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_7-atW0EsZU"
   },
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ntv0Ow89Yy2a"
   },
   "source": [
    "This notebook is intended to demonstrate some of the typical pre-processingthat occurs in NLP.  The dataset constructed will have 2 columns - a drug review and a good/bad rating.  Our assumption for a problem statement is: Can we determine the rating a person will apply based on the text in their review.  As such, it is a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4TnhbON_nY4"
   },
   "source": [
    "## Pre-requisite work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alo3dND7_t_N"
   },
   "source": [
    "\n",
    "\n",
    "1.   In a browser, navigate to https://archive-beta.ics.uci.edu/\n",
    "2.   Download the Drug Review Dataset (Drugs.com)\n",
    "\n",
    "The dataset provides patient reviews on specific drugs along with related conditions and a 10 star patient rating reflecting overall patient satisfaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcxmLldU2CEu"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r86g9B5TAlCW"
   },
   "source": [
    "### Load the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3XNaBVGj4GmF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W5B1lujRjcZd"
   },
   "outputs": [],
   "source": [
    "latuda = pd.read_csv('https://raw.githubusercontent.com/jimcody2014/nlp_cdc/main/data/latuda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I6MHmv_0_Snx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latuda.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2s2G1qpI7_sR"
   },
   "outputs": [],
   "source": [
    "!pip install contractions\n",
    "#!pip install pyspellchecker\n",
    "\n",
    "import contractions\n",
    "#from pyspellchecker import SpellChecker\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kvEK9KsFoTz"
   },
   "outputs": [],
   "source": [
    "# Add a column 'target' based on the rating column\n",
    "latuda['target'] = latuda['rating'].apply(lambda x: 'Good' if x >= 6 else 'Bad')\n",
    "latuda.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiQAR1I75IQ6"
   },
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "drop_columns = {'drugName', 'condition','date', 'usefulCount', 'rating'}\n",
    "latuda = latuda.drop(columns = drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhNIszi8dQa2"
   },
   "source": [
    "### Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcURvYlIeYJT"
   },
   "outputs": [],
   "source": [
    "latuda['remove_ctr'] = latuda['review'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "latuda.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIX8K6Gsni5U"
   },
   "outputs": [],
   "source": [
    "# change no_contract back to a string\n",
    "latuda[\"review_new\"] = [' '.join(map(str, l)) for l in latuda['remove_ctr']]\n",
    "latuda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwmhyzaioahH"
   },
   "source": [
    "#### An alternative - Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KALTz72XoavY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have had great experience so far with latuda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ive taken a lot of medications i was prescribe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i was deeply suicidal for  years with repetiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have been taking latuda  mg for  months and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weve heard of the latuda  mg restlessnesssense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>been on latuda for  weeks and seen no  improve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>i took latuda  mg for a week and then  the nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>this med did nothing for my bipolar  disorder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>i took this started at  mg caused severe restl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>started latuda several months ago  i  be happi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review\n",
       "0    i have had great experience so far with latuda...\n",
       "1    ive taken a lot of medications i was prescribe...\n",
       "2    i was deeply suicidal for  years with repetiti...\n",
       "3    i have been taking latuda  mg for  months and ...\n",
       "4    weve heard of the latuda  mg restlessnesssense...\n",
       "..                                                 ...\n",
       "325  been on latuda for  weeks and seen no  improve...\n",
       "326  i took latuda  mg for a week and then  the nex...\n",
       "327  this med did nothing for my bipolar  disorder ...\n",
       "328  i took this started at  mg caused severe restl...\n",
       "329  started latuda several months ago  i  be happi...\n",
       "\n",
       "[330 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latuda2 = pd.read_csv('https://raw.githubusercontent.com/jimcody2014/nlp_cdc/main/data/latuda.csv')\n",
    "drop_columns = {'drugName', 'condition','date', 'usefulCount', 'rating'}\n",
    "latuda2 = latuda2.drop(columns = drop_columns)\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):  \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  # word number word  \\d is number * is 0 or more occurrances\n",
    "    text = re.sub('[‘’“”…]', '', text) # remove quotes\n",
    "    text = re.sub('\\n', '', text)   # \\n - new line\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(latuda2.review.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMtquhKidUQG"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Pbr4phi4DFy"
   },
   "outputs": [],
   "source": [
    "sample = 'This is a sentence ready to be tokenized'\n",
    "print(word_tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiePvBsk5She"
   },
   "outputs": [],
   "source": [
    "sample = 'This is a paragraph ready to be tokenized.  It has two sentences.'\n",
    "print(sent_tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qHCvNNC4-ZQ"
   },
   "outputs": [],
   "source": [
    "latuda.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zwAQTfH4DIe"
   },
   "outputs": [],
   "source": [
    "latuda['tokenized'] = latuda['review_new'].apply(word_tokenize)\n",
    "latuda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1b3TVozdUZ0"
   },
   "source": [
    "### Noise cleaning (spacing, lowercase, special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGeQt7LCsuQi"
   },
   "outputs": [],
   "source": [
    "latuda['lower'] = latuda['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "latuda.head()\n",
    "\n",
    "# This could have been done before the data was tokenized.\n",
    "# latuda['lower'] = latuda['review'].str.lowercase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cumdGgdGC5ND"
   },
   "source": [
    "### Use the string package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrsOMClu9MCO"
   },
   "outputs": [],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXQsO7rM6seP"
   },
   "outputs": [],
   "source": [
    "punc = string.punctuation\n",
    "latuda['lower'] = latuda['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
    "latuda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdEQahd2-hiY"
   },
   "outputs": [],
   "source": [
    "def clean_text_round1(text):  \n",
    "    #text = text.lower()\n",
    "    #text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "\n",
    "latuda.review = pd.DataFrame(latuda.review.apply(round1))\n",
    "latuda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6QJU5YDdUqu"
   },
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KY50hxPTEXrQ"
   },
   "outputs": [],
   "source": [
    "# Import the nltk stopwords library and sewt it to english\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "latuda['no_stopwords'] = latuda['lower'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "latuda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTa7uRsWdUyP"
   },
   "source": [
    "### Stemming/ Lemmanization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtaY2knvDjHr"
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latuda['pos_tags'] = latuda['no_stopwords'].apply(nltk.tag.pos_tag)\n",
    "latuda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latuda['wordnet_pos'] = latuda['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "latuda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# There are multiple nltk stemmer methods : PorterStemmer(), SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "latuda['lemmatized'] = latuda['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "latuda.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Basic NLP Pre-processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
